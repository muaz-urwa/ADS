{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GX 6004-001:  Applied Data Science (Session 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important reason we conduct data analysis is to examine our conjectures about how the world or some system operates.  This may be done explicitly using hypothesis testing or done implicitly when we develop of a statistical model for estimation and prediction.  Implicit in the prediction model is a hypothesis about the stability of relationships through time or through space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a formalized way to conduct hypothesis testing: a data scientist lays out a conjecture about a system in a formal manner, namely a mutually exclusive and contradictory statement about the value of some parameter from a statistical model that characterizes the conjecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical hypothesis is a claim either about the value of a single population characteristic or about the values of several population characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A canonical example of hypothesis testing is a criminal trial in the **US justice system**.  The hypothesis examined by a jury is one of **'not guilty'** versus **'guilty'**.  Formally, the null hypothesis the jury uses is 'not guilty', and the alternative hypothesis is \"guilty\".  Only in the face of considerable evidence is the jury to move from the position 'not juilty' to 'juilty'.  When thinking about hypothesis testing, it is useful to keep this example in mind. \n",
    "\n",
    "In hypothesis testing, a data scientist formulates the basic proposition in a manner that one claim is initially favored (not guilty) and is rejected only in the face of sample evidence to the contrary (guilty).  The initially-favored claim is typically called the **null hypothesis**, and a data scientist rejects a null hypothesis in favor of the alternative hypothesis only in the face of **considerable evidence** to the contrary.  Otherwise, a data scientist is said to **'fail to reject'** the null hypothesis.\n",
    "\n",
    "We can write these hypotheses formally as:\n",
    "\n",
    "$H_0$: Not guilty\n",
    "\n",
    "$H_1$: Guilty\n",
    "\n",
    "The jury maintains $H_0$ at the start of the trial.  During the trial, the state (as prosecution) presents evidence to the jury that is meant to change their maintained hypothesis.  It is only in the presence of overwhelming evidence that the jury moves from $H_0$ to $H_1$.  The absence of such evidence, however, does not prompt the jury to drop their maintained hypothesis of 'not guilty'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Ingredients and Two Possible Types of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients\n",
    "1. A \"test statistic\" of some type that is only the **function of sample data**.\n",
    "2. A rejection region, based on a probability value, $\\alpha$, which is a set of values for the test statistic for which $H_0$ will be rejected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we write the test statistic as $\\hat\\theta$, which is a function only of actual data representing some population characteristic, $\\theta$.  As an example, the test statistic may be an average of some data, such as the average value of $x_1$ from our simulated data.  In this case, $\\hat\\theta=\\frac{1}{n}\\sum_i{x_i}$\n",
    "\n",
    "Frequently, we are testing against of conjecture of 0.  For example,\n",
    "\n",
    "$H_0$: $\\theta=0$\n",
    "\n",
    "$H_1$: $\\theta\\neq0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the truth, **that you do not know**, and the decision **that you must make**.  Accordingly, there are two types of possible error.\n",
    "\n",
    "Type 1 Error: Rejecting $H_0$ when it is, **in fact**, true.\n",
    "\n",
    "Type 2 Error: Failing to reject $H_0$ when it is, **in fact**, false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* Tabulate is not native to Anaconda.\n",
    "* pip install tabulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "table = [[\"H0 is:\", \" \", \"  \"],\n",
    "         [\"\", \"True\", \"False\"],\n",
    "         [\"Reject H0\", \"Type 1 error\", \" \"],\n",
    "         [\"Reject H1\", \" \", \"Type 2 error\",]]\n",
    "print(tabulate(table, tablefmt=\"fancy_grid\", numalign=\"center\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classical hypothesis testing, Pr(Type 1 error) $= \\alpha$. \n",
    "\n",
    "(Typically, in social sciences, $\\alpha$ = 0.05.  There is considerable evidence to suggest it should be much lower, say 0.01.  Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **Central Limit Theorem**, we know that test statistics, $\\hat\\theta$'s, are asymptotically distributed normal with a finite, non-zero variance (discussed below).  Therefore, the normal distribution identifies our rejection regions.  When testing, we \"z-normalize\" the test statistic.  Namely,\n",
    "\n",
    "$H_0$ : $\\theta=\\theta_0$\n",
    "\n",
    "$H_1$ : $\\theta\\neq\\theta_0$\n",
    "\n",
    "Z-normalized or t-stat: $\\displaystyle{\\frac{\\hat\\theta-\\theta_0}{SE(\\hat\\theta)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook', style='whitegrid', palette='deep', font='sans-serif', font_scale=1, rc=None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,1000)\n",
    "pdf = np.exp(-0.5*x**2)/np.sqrt(2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "plt.plot(x, pdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.ylim((0,.5))\n",
    "plt.title(r'Rejection Regions for $\\alpha=0.05$', fontsize = 20)\n",
    "plt.axvline(-1.96, color='r', ls='--', lw=2.0)\n",
    "plt.axvline(1.96, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "plt.plot(x, pdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.ylim((0,.5))\n",
    "plt.title(r'Rejection Regions for $\\alpha=0.01$', fontsize = 20)\n",
    "plt.axvline(-2.33, color='r', ls='--', lw=2.0)\n",
    "plt.axvline(2.33, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalent Statements\n",
    "\n",
    "Consider the following general hypothesis formulation.  \n",
    "\n",
    "$H_0$ : $\\theta=\\theta_0$\n",
    "\n",
    "$H_1$ : $\\theta\\neq\\theta_0$\n",
    "\n",
    "Let $SE(\\hat\\theta)$ denote the standard error of the point estimate of $\\theta$.  These are equivalent statements.\n",
    "\n",
    "1.  $\\displaystyle{\\frac{\\hat\\theta-\\theta_0}{SE(\\hat\\theta)}}>\\vert1.96\\vert$ implies a rejection of $H_0$ at a 95% level of confidence.  \n",
    "2. $\\big\\vert$t-stat$\\big\\vert$ in excess of 1.96 implies $\\hat\\theta$ is statistically different than $\\theta_0$.  (Rule of thumb, greater than 2 in absolute value.)\n",
    "3. $\\hat\\theta\\pm 1.96\\cdot SE(\\hat\\theta)$ constitutes at 95% confidence interval around $\\hat\\theta$.  If this interval includes $\\theta_0$, fail to reject $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice, we let the computer do the heavy lifting.\n",
    "\n",
    "Let's do several 'toy' examples of testing an average value against a null of zero.\n",
    "\n",
    "Sample size is $n$.  Standard error of $\\mu$ is $\\frac{\\sigma}{\\sqrt{n}}$.\n",
    "\n",
    "$H_0$ : $\\mu=0$\n",
    "\n",
    "$H_1$ : $\\mu\\neq0$\n",
    "\n",
    "#### Notes\n",
    "* Use a DGP that is Gaussian.\n",
    "* In turn, use alternative DGPs to examine the principle of hypothesis testing.\n",
    "* We will examine what happens as we increase the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "\n",
    "random.seed(1999)\n",
    "normal = np.random.normal(0, 1, n)\n",
    "print(\"The mean is %f\" % normal.mean())\n",
    "print(\"The standard error is %f\" % (normal.std()/np.sqrt(n)))\n",
    "print(\"The t-statistic is %f\" % (normal.mean()/(normal.std()/np.sqrt(n)))) \n",
    "print(\"The confidence interval is %f and %f\" % ((normal.mean()-1.96*(normal.std()/np.sqrt(n))), \n",
    "      ((normal.mean()+1.96*normal.std()/np.sqrt(n)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "random.seed(1999)\n",
    "normal = np.random.normal(1, 5, n)\n",
    "print(\"The mean is %f\" % normal.mean())\n",
    "print(\"The standard error is %f\" % (normal.std()/np.sqrt(n)))\n",
    "print(\"The t-statistic is %f\" % (normal.mean()/(normal.std()/np.sqrt(n)))) \n",
    "print(\"The confidence interval is %f and %f\" % ((normal.mean()-1.96*(normal.std()/np.sqrt(n))), \n",
    "      ((normal.mean()+1.96*normal.std()/np.sqrt(n)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "random.seed(1999)\n",
    "normal = np.random.normal(3, 1, n)\n",
    "print(\"The mean is %f\" % normal.mean())\n",
    "print(\"The standard error is %f\" % (normal.std()/np.sqrt(n)))\n",
    "print(\"The t-statistic is %f\" % (normal.mean()/(normal.std()/np.sqrt(n)))) \n",
    "print(\"The confidence interval is %f and %f\" % ((normal.mean()-1.96*(normal.std()/np.sqrt(n))), \n",
    "      ((normal.mean()+1.96*normal.std()/np.sqrt(n)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalent Statements (Again)\n",
    "\n",
    "Consider the following general hypothesis formulation.  \n",
    "\n",
    "$H_0$ : $\\theta=\\theta_0$\n",
    "\n",
    "$H_1$ : $\\theta\\neq\\theta_0$\n",
    "\n",
    "Let $SE(\\hat\\theta)$ denote the standard error of the point estimate of $\\theta$.  These are equivalent statements.\n",
    "\n",
    "1.  $\\displaystyle{\\frac{\\hat\\theta-\\theta_0}{SE(\\hat\\theta)}}>\\vert1.96\\vert$ implies a rejection of $H_0$ at a 95% level of confidence.  \n",
    "2. $\\big\\vert$t-stat$\\big\\vert$ in excess of 1.96 implies $\\hat\\theta$ is statistically different than $\\theta_0$.  (Rule of thumb, greater than 2 in absolute value.)\n",
    "3. $\\hat\\theta\\pm 1.96\\cdot SE(\\hat\\theta)$ constitutes at 95% confidence interval around $\\hat\\theta$.  If this interval includes $\\theta_0$, fail to reject $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Central Limit Theorem: Why We Test Again the Normal\n",
    "\n",
    "Start with a small number of replications and increase the number of replications. \n",
    "\n",
    "You will see the normal distribution emerge.\n",
    "\n",
    "#### This is not dependent on how we generate our samples.\n",
    "\n",
    "#### Notes\n",
    "* We draw at random from a number of distributions 100 times and calculate the average of these 100 draws.\n",
    "* We do this 500 times. \n",
    "* Then 1,000 times.\n",
    "* Then 5,000 times.\n",
    "* Then 10,000 times.\n",
    "* Then 100,000 times.\n",
    "* Then 1,000,000 times.\n",
    "* Observe how the histogram becomes the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 500\n",
    "theta = np.zeros(reps)\n",
    "bins = int(reps/100)\n",
    "\n",
    "for i in range(1, reps):\n",
    "    theta[i] = random.normal(0, 5, 100).mean()\n",
    "\n",
    "plt.figure(figsize = (12,10))\n",
    "plt.hist(theta, color='blue', density=True, bins=bins)\n",
    "plt.title('Visualizing the Central Limit Theorem', fontsize = 20)\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: Not affected by variance of Gaussian DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 500\n",
    "theta = np.zeros(reps)\n",
    "bins = int(reps/100)\n",
    "\n",
    "for i in range(1, reps):\n",
    "    theta[i] = random.normal(0, 10, 100).mean()\n",
    "\n",
    "plt.figure(figsize = (12,10))\n",
    "plt.hist(theta, color='blue', density=True, bins=bins)\n",
    "#plt.xlim(-3, 3)\n",
    "#plt.ylim(0, 0.5)\n",
    "plt.title('Visualizing the Central Limit Theorem', fontsize = 20)\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: Not affected by DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 500\n",
    "theta = np.zeros(reps)\n",
    "bins = int(reps/100)\n",
    "\n",
    "for i in range(1, reps):\n",
    "    theta[i] = random.logistic(0, 5, 100).mean()\n",
    "\n",
    "plt.figure(figsize = (12,10))\n",
    "plt.hist(theta, color='blue', density=True, bins=bins)\n",
    "#plt.xlim(-3, 3)\n",
    "#plt.ylim(0, 0.5)\n",
    "plt.title('Visualizing the Central Limit Theorem', fontsize = 20)\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 500\n",
    "theta = np.zeros(reps)\n",
    "bins = int(reps/100)\n",
    "\n",
    "for i in range(1, reps):\n",
    "    theta[i] = random.randint(1, 6, 100).mean()\n",
    "\n",
    "plt.figure(figsize = (12,10))\n",
    "plt.hist(theta, color='blue', density=True, bins=bins)\n",
    "plt.xlim(2, 4)\n",
    "#plt.ylim(0, 0.5)\n",
    "plt.title('Visualizing the Central Limit Theorem', fontsize = 20)\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: Not affected by statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 500\n",
    "theta = np.zeros(reps)\n",
    "bins = int(reps/100)\n",
    "\n",
    "for i in range(1,reps):\n",
    "    theta[i] = random.normal(0, 2, 100).std()\n",
    "\n",
    "plt.figure(figsize = (12,10))\n",
    "plt.hist(theta, color='blue', density=True, bins=bins)\n",
    "plt.xlim(1, 3)\n",
    "#plt.ylim(0, 0.5)\n",
    "plt.title('Visualizing the Central Limit Theorem', fontsize = 20)\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,1000)\n",
    "pdf = np.exp(-0.5*x**2)/np.sqrt(2*np.pi)\n",
    "\n",
    "plt.figure(figsize = (12,10))\n",
    "plt.plot(x, pdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.ylim((0,.5))\n",
    "plt.title(r'Standard Normal or Gaussian', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "plt.plot(x, pdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.ylim((0,.5))\n",
    "plt.title(r'Rejection Regions for $\\alpha=0.05$', fontsize = 20)\n",
    "plt.axvline(-1.96, color='r', ls='--', lw=2.0)\n",
    "plt.axvline(1.96, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "plt.plot(x, pdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.ylim((0,.5))\n",
    "plt.title(r'Rejection Regions for $\\alpha=0.01$', fontsize = 20)\n",
    "plt.axvline(-2.33, color='r', ls='--', lw=2.0)\n",
    "plt.axvline(2.33, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficiency: Coding Matters\n",
    "* This is not a course in coding, but sometimes it is interesting to examine ways in which efficient coding can speed calculate.\n",
    "* We will import some mathematical functions.\n",
    "* Log(1) = 0, cos(0) = 1, and += creates a sum.\n",
    "* +=cos(log(1)) is an inefficient method to create the sum of a long string of ones.\n",
    "* $\\sum_{N}1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import cos, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A loop within a loop as primitive Python code.  \n",
    "\n",
    "def f_py(I, J): \n",
    "    res = 0\n",
    "    for i in range(I):\n",
    "        for j in range (J):\n",
    "            res += int(cos(log(1)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set both loops at 10000\n",
    "# Given the calculation in the loops, we are calculating 10000 * 10000\n",
    "\n",
    "I, J = 10000, 10000\n",
    "%time res = f_py(I, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now use the more efficient Numpy arrays\n",
    "\n",
    "def f_np(I, J):\n",
    "    a = np.ones((I, J), dtype=np.float64)\n",
    "    return int(np.sum(np.cos(np.log(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time res = f_np(I, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Numba, which considerably speeds up looping.  \n",
    "# See http://numba.pydata.org/ for an explanation of how it does this.\n",
    "\n",
    "import numba as nb\n",
    "f_py_nb = nb.jit(f_py)\n",
    "f_np_nb = nb.jit(f_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f_py_nb(I, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f_np_nb(I, J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Equivalent Statements\n",
    "\n",
    "$H_0$ : $\\theta=\\theta_0$\n",
    "\n",
    "$H_1$ : $\\theta\\neq\\theta_0$\n",
    "\n",
    "Let $SE(\\hat\\theta)$ denote the standard error of the point estimate of $\\theta$.  These are equivalent statements.\n",
    "\n",
    "1.  $\\displaystyle{\\frac{\\hat\\theta-\\theta_0}{SE(\\hat\\theta)}}>\\vert1.96\\vert$ implies a rejection of $H_0$ at a 95% level of confidence.  \n",
    "2. $\\big\\vert$t-stat$\\big\\vert$ in excess of 1.96 implies $\\hat\\theta$ is statistically different than $\\theta_0$.  (Rule of thumb, greater than 2 in absolute value.)\n",
    "3. $\\hat\\theta\\pm 1.96\\cdot SE(\\hat\\theta)$ constitutes at 95% confidence interval around $\\hat\\theta$.  If this interval includes $\\theta_0$, fail to reject $H_0$. Statements\n",
    "\n",
    "$H_0$ : $\\theta=\\theta_0$\n",
    "\n",
    "$H_1$ : $\\theta\\neq\\theta_0$\n",
    "\n",
    "Let $SE(\\hat\\theta)$ denote the standard error of the point estimate of $\\theta$.  These are equivalent statements.\n",
    "\n",
    "1.  $\\displaystyle{\\frac{\\hat\\theta-\\theta_0}{SE(\\hat\\theta)}}>\\vert1.96\\vert$ implies a rejection of $H_0$ at a 95% level of confidence.  \n",
    "2. $\\hat\\theta\\pm 1.96\\cdot SE(\\hat\\theta)$ constitutes at 95% confidence interval around $\\hat\\theta$.  If this interval includes $\\theta_0$, fail to reject $H_0$.\n",
    "3. $\\big\\vert$t-stat$\\big\\vert$ in excess of 1.96 implies $\\hat\\theta$ is statistically different than $\\theta_0$.  (Rule of thumb, greater than 2 in absolute value.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Supervised Learning: Bivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a simple but straightforward approach for supervised learning: the bivariate linear model.  Why do we start with something so \"unsexy\"?  Some thoughts:\n",
    "\n",
    "1. Linear regression remains a useful and powerful tool for statistical learning and prediction.  It has been around for a very long time and is the topic of a multitude of textbooks.  \n",
    "\n",
    "2. It serves as a useful place to start as we build our toolkit of applied statistical learning techniques.  Indeed, many of the tools we will talk about are generalizations of linear regression, so a thorough understanding of it is important.\n",
    "\n",
    "3. The linear model can be expressed as a optimization problem with a closed-form solution, which makes it computationally tractible.  (You could do it by hand if you had to.)\n",
    "\n",
    "4. Empirical studies that are focused less on prediction and more on measuring the impact of a change in a feature on some outcome of interest still dominate policy-making.  Indeally, policy-makers are interested in evaluating the impact of, say, an additional year of education on wages when making policy about subsidizing education.  Is the effect different than zero?  If so, is it large and accurately measured?  Is the effect the same over different ages or cohorts?\n",
    "\n",
    "5. A linear model is a first-order Taylor approximation to a higher-order relation.  In the days of expensive computing, we convinced ourselves that so-called \"higher order terms\" or \"HOT\" were not interesting or relevant.  (We know that's not the case now.)  Besides, many of the obstacles we face in accurate statistical learning, such as omitted variable bias, impact our entire toolkit, including linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Very Brief History\n",
    "\n",
    "For succinct discussions, see https://www.utdallas.edu/~herve/Abdi-LeastSquares06-pretty.pdf and http://www.amstat.org/publications/jse/v9n3/stanton.html.\n",
    "\n",
    "Highlights: \n",
    "\n",
    "1. In the early 1800's, mathematicians Gauss and Legendre published on the Method of Least Squares.  (Gauss claims he was there first in the late 1700's.)  \n",
    "\n",
    "2. Galton published actual empirical work in 1886.\n",
    "\n",
    "3. The Bayesian/frequentist schism regarding the interpretation of statistics commences with the work of Fisher, Neyman, and the Pearsons in 1920's (to which we will return).\n",
    "\n",
    "3. One of the first complete textbooks that I know of is Edward Malinvaud, Statistical Methods of Econometrics, Rand McNally & Company, 1966."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Explanatory Model in Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the follow relation: $y=f(x)$.\n",
    "\n",
    "$x$ has several different names: \n",
    "1. an explanatory variable\n",
    "2. a predictor\n",
    "3. a feature\n",
    "\n",
    "$y$ is a quantitative response, which also has several different names:\n",
    "1. a dependent variable\n",
    "2. an outcome\n",
    "3. a label\n",
    "\n",
    "$f$ is a mapping from $x$ to $y$ that captures the relation between the two.  It could be linear, non-linear, or discontinuous.  The nature of $f$ is the supervision we provide the computer in machine learning.  To begin with, we instruct the computer that $f$ is linear.  Namely,\n",
    "\n",
    "$y=\\beta_0 + \\beta_1 \\cdot x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook', style='whitegrid', palette='deep', font='sans-serif', font_scale=1, rc=None)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = linspace(-10, 10, 1000)\n",
    "y = 1 + 2 * x\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "plt.scatter(x=x, y=y)\n",
    "plt.xlim(-10,10)\n",
    "plt.title(r'A Line, $\\beta_0=1$ and $\\beta_1=2$', fontsize=20)\n",
    "plt.xlabel(r'$x$', fontsize = 16)\n",
    "plt.ylabel(r'$y$', fontsize = 16)\n",
    "plt.axvline(0, color='r', ls='--', lw=2.0)\n",
    "plt.axhline(0, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is an abstraction that **allows us to focus**.  We do not expect from any real-world data that every possible pair $(x, y)$ to lie on this line.  Therefore, we summarize our ignorance in a fudge factor, namely:\n",
    "\n",
    "$y=\\beta_0 + \\beta_1 \\cdot x + \\epsilon$\n",
    "\n",
    "What does the $\\epsilon$ capture?\n",
    "\n",
    "1. It captures the combined effect of other features that we may or may not know.\n",
    "2. It captures the approximation error (or higher order terms).\n",
    "3. It captures the purely random component that exists at the level of an individual observation in data (i.e., fundamental unpredictabilty).  \n",
    "\n",
    "If we have a dataset of size $i=(1,...,N)$, we can write the model as:\n",
    "\n",
    "$y_i=\\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i$\n",
    "\n",
    "And this returns us to a scatterplot.\n",
    "\n",
    "** The scatterplot is all we truly have in applied data science.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = linspace(-10, 10, 10)\n",
    "y = 1 + 2 * x + np.random.normal(0, 10, 10)\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "plt.scatter(x=x, y=y)\n",
    "plt.xlim(-10,10)\n",
    "plt.ylim(-30,30)\n",
    "plt.title(r'Return to a Scatter Plot', fontsize=20)\n",
    "plt.xlabel(r'$x$', fontsize = 16)\n",
    "plt.ylabel(r'$y$', fontsize = 16)\n",
    "plt.axvline(0, color='r', ls='--', lw=2.0)\n",
    "plt.axhline(0, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and the Sum of Squared Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the real world, we start with the data.  Suppose we use a supervised linear model.  How best to fit those data?  We can imbed this into an optimization problem, namely the minimization of a convex surface, applying the principle of least squares.\n",
    "\n",
    "Denote $\\epsilon_i$ to be a residual.  Define the following convex surface, the sum of squared residuals:\n",
    "\n",
    "$SSR(\\beta_0,\\beta_1) = \\sum_i \\epsilon_i^2=\\sum_i \\left(y_i-\\beta_0 - \\beta_1 \\cdot x_i\\right)^2$\n",
    "\n",
    "The goal is to minimize the SSR with respect to $\\beta_0$ and $\\beta_1$.  That is, choose $\\hat\\beta_0$ and $\\hat\\beta_1$ such that $SSR(\\hat\\beta_0, \\hat\\beta_1$) is minimized.\n",
    "\n",
    "How does this SSR surface appear?\n",
    "\n",
    "#### Notes\n",
    "* This is a conceptual SSR surface.\n",
    "* Increase the first argument in ax.view.init() to provide a different perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = linspace(-10, 10, 100)\n",
    "b1 = linspace(-10, 10, 100)\n",
    "b0, b1 = meshgrid(b0, b1)\n",
    "rss = -1 * (-2 * b0**2 - b1**2 + b0 + b1 - 100)\n",
    "\n",
    "fig = plt.figure(figsize = (12, 10))\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_wireframe(b0, b1, rss, rstride=4, cstride=4, color='#AD5300')\n",
    "ax.view_init(20, 40)\n",
    "ax.set_title('A Conceptual SSR Surface', fontsize=20)\n",
    "ax.set_xlabel(r'$\\beta_0$', fontsize = 16)\n",
    "ax.set_ylabel(r'$\\beta_1$', fontsize = 16)\n",
    "ax.set_zlabel(r'$SSR(\\beta_0, \\beta_1)$', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\overline{x}=\\frac{1}{N}\\sum_i x_i$ be the sample mean of x and $\\overline{y}=\\frac{1}{N}\\sum_i y_i$ be the sample mean of y.  Some calculus and algebraic manipulation yields the following:\n",
    "\n",
    "$\\hat\\beta_1 = \\displaystyle{\\frac{\\sum_i(y_i-\\overline{y})(x_i-\\overline{x})}{\\sum_i(x_i-\\overline{x})^2} = \\frac{Cov(x,y)}{Var(x)}}$\n",
    "\n",
    "$\\hat\\beta_0 = \\overline{y} - \\hat\\beta_1 \\cdot \\overline{x} \\rightarrow \\overline{y}=\\hat\\beta_0 +  \\hat\\beta_1 \\cdot \\overline{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression (or \"least squares\") as a minimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "* Let's generate some fake data.  \n",
    "* obs is number of observations.\n",
    "* params is the number of parameters to be estimated ignoring the constant.  \n",
    "* It is adjusted to include a constant using statsmodels \"add_constant\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = 1000\n",
    "params = 1\n",
    "random.seed(1898)\n",
    "\n",
    "params = params + 1\n",
    "beta = np.random.randn(params, 1)\n",
    "beta0 = np.zeros((params, 1))\n",
    "X = np.random.randn(obs, params-1)\n",
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + np.random.randn(obs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* This is the linear model, $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$, to be estimated using least squares by minimization of the SSR. \n",
    "* This optimization code should look somewhat familiar.  \n",
    "* **Focus on the results, not the code.**\n",
    "* We will use the method of Monte Carlo: generate data from a **known** model and then apply least squares.\n",
    "* Evaluate how least squares performs against the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(b, y, X, obs, params):\n",
    "    bv = b.view()\n",
    "    bv.shape = params, 1\n",
    "    e = y - np.dot(X, bv)\n",
    "    return np.array(np.sum(e**2))\n",
    "\n",
    "def func_grad(b, y, X, obs, params):\n",
    "    bv = b.view()\n",
    "    bv.shape = params, 1\n",
    "    foc = -np.sum(X * (y - np.dot(X, bv)), axis=0)\n",
    "    return np.array(foc)\n",
    "\n",
    "res = minimize(func, beta0, args=(y, X, obs, params), method='BFGS', \n",
    "               jac=func_grad, options={'disp': True, 'maxiter':1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betahat = res.x.reshape((params, 1))\n",
    "e = y - np.dot(X, betahat)\n",
    "s2 = np.dot(np.transpose(e), e)/(obs - params)\n",
    "cov = s2*res.hess_inv\n",
    "\n",
    "se, t = np.zeros((params, 1)), np.zeros((params, 1))\n",
    "\n",
    "for i in range(0, params):\n",
    "    se[i] = np.sqrt(cov[i,i])\n",
    "    t[i] = res.x[i]/np.sqrt(cov[i,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The true values are\", beta)\n",
    "print(\"The fitted values are\", betahat)\n",
    "print(\"The standard errors are\", se)\n",
    "print(\"The t-stats are\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimization Exercise\n",
    "\n",
    "The Python code above expresses the bivariate model as a minimization problem.  Run the code a few times to demonstrate that the model minimizes without any issues.  For example, increase the sample size.  What happens to the t-statistics as the sample size grows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Get Real: Make the Machine Do It!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(pd.DataFrame(y), pd.DataFrame(X), left_index=True, right_index=True)\n",
    "\n",
    "mod = smf.ols(formula='y ~ X - 1', data = data).fit()\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The true values are\", beta)\n",
    "print(\"The fitted values are\", betahat)\n",
    "print(\"The standard errors are\", se)\n",
    "print(\"The t-stats are\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Application: The Capital Asset Pricing Model (CAPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAPM remains one of the workhorse statistical models in finance.  It is so common that sites like Yahoo!Finance report the measures we will estimate statistically.  \n",
    "\n",
    "**I use CAPM for a reason: absent experimental data, it is the only application of the bivariate linear model that I believe has validity in the real world.**\n",
    "\n",
    "The upshot of the CAPM says that we can directly interpret the intercept and slope coefficients as real-world measures relating the excess (or risk-adjusted) returns of a particular stock to those of a basket of stocks, such as a specific market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPM: Regression and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $r_i$ denote the rate of return to asset $i$, $r_f$ denote the known rate of return on a risk-free asset (for example, short-term US government debt), and $r_m$ denote the rate of return to a portfolio (or a market) that includes $i$.  \n",
    "\n",
    "The following equation can be derived:\n",
    "\n",
    "$E(r_i)-r_f = \\alpha + \\beta \\cdot (E(r_m)-r_f)$\n",
    "\n",
    "Here, $E(\\cdot)$ denotes an expected value (because this is a forward-looking prediction).  In CAPM, $\\beta$ captures the sensitivity of an asset’s returns to the returns to a portfolio or to the market on which an asset trades.  In other words, it non-diversifiable risk.  In addition, $\\alpha$ measures an asset $i$’s excess (or abnormal) returns.\n",
    "\n",
    "Phrases you may have heard: \n",
    "\n",
    "1. The stock's beta is high\n",
    "2. The hedge fund is chasing alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we can express our conjecture as a joint hypothesis:\n",
    "\n",
    "$H_0: \\alpha = 0, \\beta=1$\n",
    "\n",
    "$H_1: \\text{ not }H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Look at the Obvious: Teaching in a Sea of These\n",
    "\n",
    "#### Notes\n",
    "* This used to be easy until the death of the Yahoo!Finance API.\n",
    "* pip install fix_yahoo_finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from PIL import Image\n",
    "import urllib.request as url\n",
    "import io\n",
    "\n",
    "fd = url.urlopen(\"http://buzzmybiz.co/wp-content/uploads/2012/06/apple-logo.jpg\")\n",
    "image_file = io.BytesIO(fd.read())\n",
    "Image.open(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt  \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook', style='whitegrid', palette='deep', font='sans-serif', font_scale=1, rc=None)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import fix_yahoo_finance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab Apple and NASDAQ data.  \n",
    "\n",
    "start, end = \"2006-01-01\", \"2016-12-31\"\n",
    "\n",
    "aapl_all = yf.download('aapl', start=start, end=end)\n",
    "nasdaq_all = yf.download('^ixic', start=start, end=end)\n",
    "aapl = aapl_all['Adj Close']\n",
    "nasdaq = nasdaq_all['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "aapl.plot(color='blue')\n",
    "plt.title('AAPL ($/Share)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "nasdaq.plot(color='blue')\n",
    "plt.title('NASDAQ', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate log returns, which is the standard finance measure for daily returns in finance.\n",
    "\n",
    "aapl_returns = (np.log(aapl / aapl.shift(1))).dropna()\n",
    "nasdaq_returns = (np.log(nasdaq / nasdaq.shift(1))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average daily AAPL returns and 'risk' are %f and %f\" % (aapl_returns.mean(), aapl_returns.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.hist(aapl_returns, bins=150, density=True, color='blue')\n",
    "plt.title('Histogram of AAPL Daily Returns', fontsize=20)\n",
    "plt.ylabel('%', fontsize=8)\n",
    "plt.axvline(0, color='red')\n",
    "plt.xlim(-0.2, 0.2)\n",
    "plt.ylim(0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average daily market returns and 'risk' are %f and %f\" % (nasdaq_returns.mean(), nasdaq_returns.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.hist(nasdaq_returns, bins=150, density=True, color='blue')\n",
    "plt.title('Histogram of NASDAQ Daily Returns', fontsize=20)\n",
    "plt.ylabel('%', fontsize=8)\n",
    "plt.axvline(0, color='red')\n",
    "plt.xlim(-0.2, 0.2)\n",
    "plt.ylim(0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aapl_returns = pd.DataFrame(aapl_returns)\n",
    "nasdaq_returns = pd.DataFrame(nasdaq_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "plt.scatter(aapl_returns, nasdaq_returns, c='b')\n",
    "plt.title('CAPM Data', fontsize = 20)\n",
    "plt.xlabel('Log Returns of NASDAQ', fontsize = 10)\n",
    "plt.ylabel('Log Returns of AAPL', fontsize = 10)\n",
    "plt.xlim([-0.1, 0.1])\n",
    "plt.ylim([-0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* Merge and rename.\n",
    "* We create two dataframes using Pandas.\n",
    "* We then merge the two dataframes using the indexes associated with each frame.  \n",
    "* You will be able to use this code repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_r = pd.DataFrame(aapl_returns)\n",
    "nasdaq_r = pd.DataFrame(nasdaq_returns)\n",
    "data = pd.merge(nasdaq_r, aapl_r, left_index=True, right_index=True)\n",
    "data.rename(columns={'Adj Close_x':'nasdaq', 'Adj Close_y':'aapl'}, inplace=True)\n",
    "mod = smf.ols(formula='aapl ~ nasdaq', data = data).fit()\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(12 ,10))\n",
    "ax.scatter(aapl_returns, nasdaq_returns, c=\"b\")\n",
    "mod = smf.ols(formula='aapl ~ nasdaq', data = data).fit()\n",
    "abline_plot(model_results=mod, ax=ax, color='red')\n",
    "\n",
    "ax.set_title('CAPM Data', fontsize = 20)\n",
    "ax.set_ylabel('Log Returns of AAPL', fontsize = 10)\n",
    "ax.set_xlabel('Log Returns of NASDAQ', fontsize = 10)\n",
    "ax.set_xlim([-0.1, 0.1])\n",
    "ax.set_ylim([-0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPM Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find another tech stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = \"2008-08-31\", \"2018-08-31\"\n",
    "\n",
    "goog_all = yf.download('goog', start=start, end=end)\n",
    "nasdaq_all = yf.download('^ixic', start=start, end=end)\n",
    "goog = goog_all['Adj Close']\n",
    "nasdaq = nasdaq_all['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "goog.plot(color='blue')\n",
    "plt.title('GOOG ($/Share)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "nasdaq.plot(color='blue')\n",
    "plt.title('NASDAQ', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goog_returns = (np.log(goog / goog.shift(1))).dropna()\n",
    "nasdaq_returns = (np.log(nasdaq / nasdaq.shift(1))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.hist(goog_returns, bins=150, density=True, color='b')\n",
    "plt.title('Histogram of GOOG Daily Returns', fontsize=20)\n",
    "plt.ylabel('%', fontsize=8)\n",
    "plt.axvline(0, color='red')\n",
    "plt.xlim(-0.2, 0.2)\n",
    "plt.ylim(0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.hist(nasdaq_returns, bins=150, density=True, color='blue')\n",
    "plt.title('Histogram of NASDAQ Daily Returns', fontsize=20)\n",
    "plt.ylabel('%', fontsize=8)\n",
    "plt.axvline(0, color='red')\n",
    "plt.xlim(-0.2, 0.2)\n",
    "plt.ylim(0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog_returns = pd.DataFrame(goog_returns)\n",
    "nasdaq_returns = pd.DataFrame(nasdaq_returns)\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "plt.scatter(goog_returns, nasdaq_returns, c='b')\n",
    "plt.title('CAPM Data', fontsize = 20)\n",
    "plt.xlabel('Log Returns of NASDAQ', fontsize = 10)\n",
    "plt.ylabel('Log Returns of GOOG', fontsize = 10)\n",
    "plt.xlim([-0.1, 0.1])\n",
    "plt.ylim([-0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(nasdaq_returns, goog_returns, left_index=True, right_index=True)\n",
    "data.rename(columns={'Adj Close_x':'nasdaq', 'Adj Close_y':'goog'}, inplace=True)\n",
    "mod = smf.ols(formula='goog ~ nasdaq', data = data).fit()\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(12 ,10))\n",
    "ax.scatter(goog_returns, nasdaq_returns, c=\"b\")\n",
    "mod = smf.ols(formula='goog ~ nasdaq', data = data).fit()\n",
    "abline_plot(model_results=mod, ax=ax, color='red')\n",
    "\n",
    "ax.set_title('CAPM Data', fontsize = 20)\n",
    "ax.set_ylabel('Log Returns of GOOG', fontsize = 10)\n",
    "ax.set_xlabel('Log Returns of NASDAQ', fontsize = 10)\n",
    "ax.set_xlim([-0.1, 0.1])\n",
    "ax.set_ylim([-0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splits and (Out-of-Sample) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The discussion above is focused on a traditional approach to supervised learning: use all of the data.\n",
    "* Now we will focus on splitting our data into a training set and a test set.  \n",
    "* We do not let the algorithm see the test set.\n",
    "* We use the test set to evaluate the performance of the trained algorithm.\n",
    "* Mean squared error: $\\sum \\{\\hat{y} - y\\}^2$ over test set and average.\n",
    "* Mean squared forecast error in time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have the Google CAPM data loaded, so let's us it.\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split\n",
    "\n",
    "* I used to teach how to do this 'by hand'.  \n",
    "* Conceptually straightforward.  \n",
    "* Suppose I wanted to split **randomly** a dataset of size $N$ into a training set of $0.80\\cdot N \\text{ and }0.02\\cdot  N$.\n",
    "* Use a weighted coin.\n",
    "* Now can use another library, Scikit Learning, a Python machine learning libary.\n",
    "* train_test_split is the object that splits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Python instead.\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatrain, datatest = train_test_split(data, test_size = 0.2, random_state = 6281993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatest.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the linear model on the training set.  Created fitted values on the test set.\n",
    "\n",
    "mod = smf.ols(formula='goog ~ nasdaq', data = datatrain).fit()\n",
    "datatest['fitted'] = mod.predict(exog = datatest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline against which to measure.\n",
    "\n",
    "print('The average in-sample GOOG return is %f' % datatrain['goog'].mean())\n",
    "print('The average out-sample GOOG return is %f' % datatest['goog'].mean())\n",
    "print('The average predicted GOOG return is %f' % datatest['fitted'].mean())\n",
    "print('The MSE is %f' % ((datatest['goog'] - datatest['fitted'])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(12 ,10))\n",
    "ax.scatter(datatest['goog'], datatest['nasdaq'], c=\"b\")\n",
    "mod = smf.ols(formula='goog ~ nasdaq', data = datatrain).fit()\n",
    "abline_plot(model_results=mod, ax=ax, color='red')\n",
    "\n",
    "ax.set_title('Out of Sample Performance', fontsize = 20)\n",
    "ax.set_ylabel('Log Returns of GOOG', fontsize = 10)\n",
    "ax.set_xlabel('Log Returns of NASDAQ', fontsize = 10)\n",
    "ax.set_xlim([-0.1, 0.1])\n",
    "ax.set_ylim([-0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### In this use case, what does the red line represent?\n",
    "#### In this use case, what do points above the red line represent?\n",
    "#### In this use case, what do points below the red line represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Will Return to All of These Concepts for both Bayesian Inference and Deep Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
