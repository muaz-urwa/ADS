{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GX 6004-001:  Applied Data Science (Session 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data science is uniting researchers from a variety of disciplines.\n",
    "* Physical, computer and social scientists, each bringing both strengths and biases.\n",
    "* The golden age of empirical analysis (for good or otherwise).\n",
    "* CUSP and NYU are emerging as the nexus of this 'consilience'.\n",
    "* This course will reflect a multidisciplinary approach (with my biases as a Bayesian econometrician)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Arc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transparency and honesty.\n",
    "* Data science did __NOT__ begin in 2010.\n",
    "* Not econometrics, statistics, or machine learning.\n",
    "* A synthesis of important elements of each of these, as data science is meant to combine each.\n",
    "* Strict emphasis on the applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Expose you to a variety of tools for data analytics.\n",
    "    * Linear and logistic regression are the indepensible foundation of applied data science.\n",
    "    * Time series analysis has unique statistical features and is typically not featured in machine learning courses.\n",
    "    * Bayesian inference has emerged as a critical data science tool.\n",
    "    * Deep learning has greatly expanded the solution set.\n",
    "    * Yes, this is a lot of stuff.\n",
    "* Eliminate inherent advantages where possible through the use of the Jupyter environment and Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Processes, Causality and Interpretation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Great Depression and Data Science: An Object Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prior to the 1930s, there were few government-sponsored measures of economic activity.\n",
    "* As the US and Europe slid into the Great Depression in the early 1930's, policy makers lacked basic information. \n",
    "* In the US, the National Accounts were born in 1934 and greatly expanded during and after WWII.\n",
    "* At the same time, Alfred Cowles established the Cowles Commission for Research in Economics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cowles Commission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cowles approach was a probabilistic framework to estimate systems of simultaneous equations to model an economy.\n",
    "* Ultimately would develop large scale machine learning models to examine a host of different economic variables.\n",
    "* Cowles was big data of the day (and of a sort).\n",
    "* Key insights not lost to this day. \n",
    "* Approach was ultimately found to be inadequate for detailed policy evaluation.\n",
    "    * “Goodhart’s law” and “Lucas critique”.\n",
    "    * Nevertheless, they were part of the 'critical path' of tool development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodhart and Lucas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Goodhart “asserts that any economic relation tends to break down when used for policy purposes.” Wickens [2008].\n",
    "    * Here, \"policy purposes\" could be broadly interprested as 'any use in practice'.\n",
    "* Proposed relationships, economic or otherwise, are not structural in nature.\n",
    "* Instead derived from fundamental behavioral relationships (the structure of the system).\n",
    "* Lucas (1976) notes that individual decision rules affected by policy are driven by “deep structural parameters.” \n",
    "* Decision rules and, therefore, decisions are contingent on the state of the system **as it is**.\n",
    "* Change the system through policy, change the decision rule.\n",
    "    * We have developed techniques to conduct experiments, such as A/B testing and conjoint analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure and Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data are typically non-experimental.\n",
    "* The digital exhaust of human activity.\n",
    "* As a result, analysis is subject to potential selection bias, as well as the critiques of Goodhard and Lucas.\n",
    "* Development of techniques to deal with such bias. \n",
    "* Moreover, direction of causation must be clearly understood: 'umbrellas cause rain'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Goals for the Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm: Representation, Evaluation and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation\n",
    "\n",
    "* $y = m * x + b$\n",
    "\n",
    "* $y = f(\\text{observed features})$\n",
    "\n",
    "* $y = x'\\beta + \\epsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "* Depends on the use case.\n",
    "    * Evaluation of impact of an intervention: hypothesis testing using either classical or Bayesian inference.\n",
    "    * Prediction: what is the likelihood (or probability) that this unlabeled number is a zero?\n",
    "    * Time-series forecast: mean-squared forecast error on an interest rate forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine-learning algorithms optimize an objective function.  For example, least squares minimizes its objective function while the logit classifier maximizes its objective function.  Objective functions may have \"nice\" properties that make optimization easy, but problems may arise when properties are not global.  (And we lack closed-form proofs of global optima.)  \n",
    "\n",
    "Let's examine a couple of optimization problems so that you have an idea of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canonical set up:\n",
    "\n",
    "$f: \\Re^{N} \\rightarrow \\Re$\n",
    "\n",
    "Goal is to find those points $x \\in \\Re^{N}$ at which $f(x)$ takes on a critical value.  We can write this as unconstrained problems as\n",
    "\n",
    "$\\max_{x} f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary first order conditions (FOCs) are:\n",
    "\n",
    "${\\displaystyle \\frac{\\partial f(x)}{\\partial x_i} = 0} \\forall i$ \n",
    "\n",
    "The sufficient secord order conditions (SOCs) are:\n",
    "\n",
    "${\\displaystyle \\frac{\\partial^2 f(x)}{\\partial x_i\\partial x_j}}$\n",
    "is $\\left\\{ \\begin{array}{l l} \n",
    "{} & \\quad \\text{Negative definite if a maximum} \\\\ \n",
    "{} & \\quad \\text{Postive definite if a miminum} \\\\\n",
    "{} & \\quad \\text{Indeterminate if neither} \\\\\n",
    "\\end{array} \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "$f(x_1,x_2)=-2x_1^2-x_2^2 + x_1 + x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "* We typically import various Python libraries, in this instance for graphing and optimization.\n",
    "* matplotlib is a general Python graphing libary, similar to R's ggplot2.\n",
    "* seaborn is \"... a library for making statistical graphics in Python.\" \n",
    "* \"It is built on top of matplotlib and closely integrated with pandas data structures.\"\n",
    "* Minimize is a function called from SciPy, a scientific computing library in Python. \n",
    "* We will create arrays to mirror the real line, using linspace.\n",
    "* We will then graph using two different approaches, the wire mesh and the heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook', style='whitegrid', palette='deep', font='sans-serif', font_scale=1, rc=None)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = linspace(-10,10,100)\n",
    "x2 = linspace(-10,10,100)\n",
    "x1, x2 = meshgrid(x1, x2)\n",
    "f = -2 * x1**2 - x2**2 + x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 10))\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_wireframe(x1, x2, f, rstride=4, cstride=4, color='#AD5300')\n",
    "ax.view_init(20,50)\n",
    "ax.set_xlabel(r'$x_1$', fontsize = 16)\n",
    "ax.set_ylabel(r'$x_2$', fontsize = 16)\n",
    "ax.set_zlabel(r'$f(x_1, x_2)$', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 10))\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(x1, x2, f, rstride=2, cstride=2, cmap=cm.coolwarm, shade='interp')\n",
    "ax.view_init(20,50)\n",
    "ax.set_xlabel(r'$x_1$', fontsize = 16)\n",
    "ax.set_ylabel(r'$x_2$', fontsize = 16)\n",
    "ax.set_zlabel(r'$f(x_1, x_2)$', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* This is a typical method of Python programming.\n",
    "* We define a function, func, to which we pass objects.\n",
    "* We write it to return manipulations of those objections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(params, sign=1.0):\n",
    "    x1, x2 = params\n",
    "    return sign*(-2 * x1**2 - x2**2 + x1 + x2)\n",
    "\n",
    "minimize(func, [-10.0, -10.0], args=(-1.0,), method='BFGS', options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "$f(x_1,x_2)= - \\sqrt{x_1^2+x_2^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = linspace(-10,10,100)\n",
    "x2 = linspace(-10,10,100)\n",
    "x1, x2 = meshgrid(x1, x2)\n",
    "f = -1.0 * sqrt(x1**2 + x2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 10))\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_wireframe(x1, x2, f, rstride=4, cstride=4, color='#AD5300')\n",
    "ax.view_init(20,50)\n",
    "ax.set_xlabel(r'$x_1$', fontsize = 16)\n",
    "ax.set_ylabel(r'$x_2$', fontsize = 16)\n",
    "ax.set_zlabel(r'$f(x_1, x_2)$', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 10))\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(x1, x2, f, rstride=2, cstride=2, cmap=cm.coolwarm, shade='interp')\n",
    "ax.view_init(20,50)\n",
    "ax.set_xlabel(r'$x_1$', fontsize = 16)\n",
    "ax.set_ylabel(r'$x_2$', fontsize = 16)\n",
    "ax.set_zlabel(r'$f(x_1, x_2)$', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(params, sign=1.0):\n",
    "    x1, x2 = params\n",
    "    return sign*(sqrt(x1**2 + x2**2))\n",
    "\n",
    "minimize(func, [-10.0, 10.0], args=(1.0,), method='BFGS', options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Exercise\n",
    "\n",
    "Let $f(x_1,x_2)=x_2^2-x_1^2$\n",
    "\n",
    "Graph $f(x)$ over a relevant domain using Python.  Find all critical points and determine their nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Topics from Probability and Statistics\n",
    "\n",
    "### What is probability?  \n",
    "\n",
    "Classical view arose from gambling with dice and holds that outcomes have equal probabilty.  \n",
    "\n",
    "Subjective view uses a model with randomness such as the payoff to a particular gamble.  \n",
    "\n",
    "Classical view holds that probability is based on the history of outcomes from an experiment, such as the probability the stock market goes up tomorrow or P(stock market goes up tomorrow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of an Experiment\n",
    "\n",
    "#### Sample space, $S$, which is the set of all possible outcomes.\n",
    "\n",
    "Examples: \n",
    "\n",
    "1. Flipping a US penny, $S=\\{Heads,Tails\\}$\n",
    "\n",
    "2. Throwing a die, $S=\\{1,2,3,4,5,6\\}$\n",
    "\n",
    "3. Throwing two dice, $S=\\{i,j\\}: i,j=1,2,3,4,5,6$\n",
    "\n",
    "#### Events, $A$, which are any subset of $S$\n",
    "\n",
    "Examples: \n",
    "\n",
    "1. $Heads \\text{ from } \\{Heads,Tails\\}$\n",
    "\n",
    "2. $2 \\text{ from } \\{1,2,3,4,5,6\\}$\n",
    "\n",
    "#### Probability \n",
    "\n",
    "$P: A\\rightarrow[0,1]$ or $P(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properites of Probability\n",
    "\n",
    "1. The probability of an event occuring lies between 0 and 1: $P(A)\\in[0,1]$\n",
    "\n",
    "2. The probability of the sample space occurring is 1: $P(S)=1$\n",
    "\n",
    "3. Summation: $P(A \\bigcup B) = P(A) + P(B)$ for independent events\n",
    "\n",
    "4. Conditioning: $P(A|B) = P(A)$ for independent events\n",
    "\n",
    "5. Complimentary: $P(A^c) = 1-P(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n",
    "\n",
    "We operationalize all of this through the use of Random Variables (both discrete and continuous) and examine important characteristics of these animals (moments) such as mean, variance (standard error), and correlation (when we have more than one RV).\n",
    "\n",
    "### Examples of Discrete RVs\n",
    "Experiment 1 is to toss a coin 25 times.  The random variable is the outcome, $\\{Heads, Tails\\}$, from which we can calculate a mean and a standard deviation, among other things.\n",
    "\n",
    "Experiment 2 is to toss two dice 10 times.  The random variables are the outcomes of each toss, from which we can calculate a mean, a standard deviation, and a correlation.  Technically, this is a binomial random variable (multiple experiments of a single Bernoulli trial).\n",
    "\n",
    "Let's make this practical and do some statistical computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1\n",
    "* Flip a US penny 25 times.\n",
    "* Record the outcome as 0 = tails and 1 = heads.\n",
    "* stats is a statistical libary from SciPy.\n",
    "* Note that we create a random seed which allows us to replicate sequences of random draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "from scipy import stats\n",
    "\n",
    "random.seed(2018)\n",
    "sample = random.randint(2, size=25)\n",
    "cov = std(sample) / mean(sample)\n",
    "sr = mean(sample) / std(sample)\n",
    "print(\"The sequence is %s\" % sample)\n",
    "print(\"The mean is %f\" % mean(sample))\n",
    "print(\"The variance is %f\" % var(sample))\n",
    "print(\"The standard deviation is %f\" % std(sample))\n",
    "print(\"The coefficient of variation is %f\" % cov)\n",
    "print(\"The Sharpe ratio is %f\" % sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematics of Summary Statistics\n",
    "\n",
    "Let X denote a Random Variable and x denote the event.  We seek to summarize important components about it: feature reduction.\n",
    "\n",
    "Average / Expectation / Mean:\n",
    "\n",
    "$\\mu=E[X]=\\sum P(X=x) \\cdot x$\n",
    "\n",
    "Variance:\n",
    "\n",
    "$\\sigma^2=var[X]=E[(X-E[X])^2]=\\sum_k P(X=x) \\cdot (x-\\mu)^2$\n",
    "\n",
    "Standard deviation:\n",
    "\n",
    "$\\sigma=\\sqrt{\\sigma^2}$\n",
    "\n",
    "Coefficient of variation:\n",
    "\n",
    "$c_v=\\frac{\\sigma}{\\mu}$\n",
    "\n",
    "Sharpe Ratio:\n",
    "\n",
    "$S_r=\\frac{\\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2\n",
    "* Role two six-sided dice.\n",
    "* Record the outcome of the face.\n",
    "* Correlation: the degree to which to random variables move together.  Bounded between [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1492)\n",
    "die1 = random.randint(1, 6, size=10)\n",
    "die2 = random.randint(1, 6, size=10)\n",
    "\n",
    "print(\"The sequences are %s and %s\" % (die1, die2))\n",
    "print(\"The means and standard deviations are %f, %f, %f and %f\" % (die1.mean(), die2.mean(), die1.std(), die2.std()))\n",
    "print(\"The correlation between the two dice is %f\" % np.corrcoef(die1, die2)[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's change the scope of the experiment.\n",
    "* Flip a US penny 25 times and ask, \"What is the probability that I get $x$ number of heads out of 25 flips?\"\n",
    "* Bernoulli RV gives rise to binomial RV.\n",
    "* Note the 'bell shape' of the histogram of the outcomes.\n",
    "* Getting 12 heads out of 25 flips is highly probable.\n",
    "* Getting 2 heads out of 25 flips is hihgly improbable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "k=range(0, 25)\n",
    "plt.bar(k, stats.binom.pmf(k, p=0.5, n=25))\n",
    "plt.xlabel('Event', fontsize = 16)\n",
    "plt.ylabel('Probability of Event', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Discrete RVs\n",
    "\n",
    "1. Multinomial (multiple outcomes, such as position A, B, C, or D).\n",
    "2. Poisson (positive and integer-valued, often used for counting, such as the number of taxi cabs observed at 23rd and 5th in an hour)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asymptotics: What Happens As We Increase the Number of Experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "k=range(0, 250)\n",
    "plt.bar(k,stats.binom.pmf(k, p=0.5, n=250))\n",
    "plt.xlabel('Event', fontsize = 16)\n",
    "plt.ylabel('Probability of Event', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Important Continuous RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic\n",
    "\n",
    "Cumulative Density Function\n",
    "\n",
    "$F(x)=\\displaystyle{\\frac{1}{1+\\exp(-x)}}$\n",
    "\n",
    "Probability Density Function\n",
    "\n",
    "$f(x)=F^\\prime(x)=\\displaystyle{\\frac{\\exp(-x)}{1+\\exp(-x)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "pdf = np.exp(-x)/(1+np.exp(-x))**2\n",
    "cdf = 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "plt.plot(x, cdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.title('Logistic CDF', fontsize = 20)\n",
    "plt.axvline(0, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "plt.plot(x, pdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.ylim((0,.3))\n",
    "plt.title('Logistic PDF', fontsize = 20)\n",
    "plt.axvline(0, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Digression\n",
    "\n",
    "The following relations can be used to show that the logistic RV meets the properties of probability.\n",
    "\n",
    "$x^0=1 \\text{ } \\forall x\\in\\Re$\n",
    "\n",
    "$\\lim_{x\\rightarrow\\inf}\\frac{1}{x}=0$\n",
    "\n",
    "$\\lim_{x\\rightarrow\\inf}\\exp(x)=\\infty$\n",
    "\n",
    "$\\lim_{x\\rightarrow-\\inf}\\exp(x)=0$\n",
    "\n",
    "$\\lim_{x\\rightarrow\\inf}\\alpha^x=0 \\text{ for } \\mid\\alpha\\mid<1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Standard Normal (or Gaussian), denoted $N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability Density Function\n",
    "\n",
    "$f(x)=\\displaystyle{\\frac{1}{\\sqrt{2\\pi}}}\\exp\\{-\\frac{1}{2}x^2\\}$\n",
    "\n",
    "For $N(\\mu,\\sigma^2)$ formula is:\n",
    "\n",
    "$f(x)=\\displaystyle{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}}\\exp\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,1000)\n",
    "pdf = np.exp(-0.5*x**2)/np.sqrt(2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "plt.plot(x, pdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.ylim((0,.5))\n",
    "plt.title('Standard Normal PDF', fontsize = 20)\n",
    "plt.axvline(0, color='r', ls='--', lw=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's up with the $\\sqrt{2\\pi}$ in the Normal?\n",
    "\n",
    "For the $N(\\mu,\\sigma^2)$ random variable, the PDF is:\n",
    "\n",
    "$f(x)=\\displaystyle{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}}\\exp\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\}$\n",
    "\n",
    "We know that that CDF does not have a closed-form solution.  Again, the action is in $\\exp\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\}$.  For ease, set the mean to 0 and the variance to 1.  The action is in $\\exp\\{-\\frac{1}{2}x^2\\}$. The integral over the entire support of this function is greater than one.  To make it a proper distribution, we have to normalize it so that the integral is one, which is the role of $\\sqrt{2\\pi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Continnous RVs\n",
    "\n",
    "1. Student's t distribution (for small-sample hypothesis testing).\n",
    "2. Uniform over a range (for random number generation).\n",
    "3. Chi-Squared, the square of the normal (for joint hypothesis testing).\n",
    "4. Log normal, the transformation of non-negative things like wages or stock returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Normal with Correlation $\\rho$\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Multivariate_normal_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "x, y = np.linspace(-4, 4, 100), np.linspace(-4, 4, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = np.mat(np.zeros(2))\n",
    "p = np.zeros(np.shape(x))\n",
    "R = np.matrix([[1, 0.5],[0.5, 1]])\n",
    "Rinv = linalg.inv(R)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        z[0, 0] = x[i, j]\n",
    "        z[0, 1] = y[i, j]\n",
    "        p[i, j] = (1.0 / (2 * np.pi) * np.sqrt(linalg.det(R))) * np.exp(-(z * Rinv *z.T) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 10))\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(x, y, p, rstride=2, cstride=2, cmap=cm.coolwarm, shade='interp')\n",
    "ax.view_init(20, 80) # Rotate through 20, 40, 60, 80\n",
    "ax.set_xlabel(r'$x_1$', fontsize = 10)\n",
    "ax.set_ylabel(r'$x_2$', fontsize = 10)\n",
    "ax.set_zlabel(r'$f(x_1, x_2)$', fontsize = 10)\n",
    "ax.set_title(r'Bivariate Normal, $\\rho = 0.5$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* In the heat plot, red indicates high probability of occurrence\n",
    "* Blue indicates a low probability of occurrence.  \n",
    "* In other words, values near zero are quite probable, while values far from zero are quite improbable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 10))\n",
    "ax = fig.gca()\n",
    "ax.contour(x, y, p)  \n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_xlabel(r'$x$', fontsize = 10)\n",
    "ax.set_ylabel(r'$y$', fontsize = 10)\n",
    "ax.set_title(r'Contour Plot of Bivariate Normal, $\\rho = 0.5$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sample Data, Important Moments, and Scatterplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, we don't observe the random variables.  Rather we observe data that arises from some underlying data generating process (DGP).  We use the concept of a random variable to operationalize our analytics.  \n",
    "\n",
    "* In applied data science (or statistical learning), we often work with structured data.\n",
    "* We undertake an analysis of those data.  \n",
    "* Often this is form of \"dimension reduction,\" in that we seek to develop lower-dimensional but informative representations of the data.  \n",
    "* (We will discuss unstructured data later in the course with an introduction to deep learning.)\n",
    "\n",
    "**Again, we will allow the computer to do the heavy lifting for us.**  \n",
    "\n",
    "Let's start generating some random samples and playing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1066)\n",
    "size = 1000 # Sample size of the observed data\n",
    "mean = [0, 0] # Mean zero\n",
    "corr = 0.5 # Correlation 0.5\n",
    "R = np.matrix([[1, corr],[corr, 1]]) \n",
    "x1, x2 = np.random.multivariate_normal(mean, R, size).T # Draw the observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The summary statistics are %f, %f and %f\" % ((x1.mean(), x1.var(), x1.std())))\n",
    "print(\"The summary statistics are %f, %f and %f\" % ((x2.mean(), x2.var(), x2.std())))\n",
    "print(\"The correlation is %f\" % np.corrcoef(x1, x2)[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(x1, x2, 'bo')\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.xlabel(r'$x_1$', fontsize = 16)\n",
    "plt.ylabel(r'$x_2$', fontsize = 16)\n",
    "plt.title(r'Scatterplot of Simulated Data', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* In the real world, we observe the scatterplot above and seek to interpret it.\n",
    "* We can calculate summary statistics, as above.\n",
    "* How else might we summarize?  (Gauss figured this out a long time ago.)\n",
    "* We will review the code below in much greater detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "import patsy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.merge(pd.DataFrame(x1), pd.DataFrame(x2), left_index=True, right_index=True)\n",
    "data.rename(columns={'0_x':'x1', '0_y':'x2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.scatter(x1, x2, c='b')\n",
    "mod = smf.ols(formula='x1 ~ x2', data = data).fit()\n",
    "abline_plot(model_results=mod, ax=ax, color='red')\n",
    "\n",
    "plt.xlabel(r'$x_1$', fontsize = 16)\n",
    "plt.ylabel(r'$x_2$', fontsize = 16)\n",
    "plt.title(r'Scatterplot of Simulated Data', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve Fitting Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we know our data are generated from a bivariate normal with correlation that is very high and positive.  What would a scatter plot display?  What line might best fit these data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1066)\n",
    "size = 1000 # Sample size of the observed data\n",
    "mean = [0, 0] # Mean zero\n",
    "corr = 0.9 # Correlation 0.9\n",
    "R = np.matrix([[1, corr],[corr, 1]]) \n",
    "x1, x2 = np.random.multivariate_normal(mean, R, size).T # Draw the observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(x1, x2, 'bo')\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.xlabel(r'$x_1$', fontsize = 16)\n",
    "plt.ylabel(r'$x_2$', fontsize = 16)\n",
    "plt.title(r'Scatterplot of Simulated Data', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = pd.DataFrame(x1)\n",
    "x2 = pd.DataFrame(x2)\n",
    "data = pd.merge(x1, x2, left_index=True, right_index=True)\n",
    "data.rename(columns={'0_x':'x1', '0_y':'x2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.scatter(x1, x2, c='b')\n",
    "mod = smf.ols(formula='x1 ~ x2', data = data).fit()\n",
    "abline_plot(model_results=mod, ax=ax, color='red')\n",
    "\n",
    "plt.xlabel(r'$x_1$', fontsize = 16)\n",
    "plt.ylabel(r'$x_2$', fontsize = 16)\n",
    "plt.title(r'Scatterplot of Simulated Data', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve Fitting Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we know our data are generated from a bivariate normal with correlation that is very high and negative.  What would a scatter plot display?  What line might best fit these data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Hypothesis Testing\n",
    "\n",
    "* Algorithms and hypothesis testing are conceptual different things.\n",
    "    * Develop an algorithm that has no underlying hypothesis to test.\n",
    "    * Develop a hypothesis without an underlying algorithm to examine.\n",
    "* One important reason we conduct data analysis is to examine our conjectures about how the world or some system operates. \n",
    "* This may be done explicitly using hypothesis testing or done implicitly when we develop of a statistical model for estimation and prediction. \n",
    "    * Implicit in the prediction model is a hypothesis about the stability of relationships through time or through space.\n",
    "* There is a formalized way to conduct hypothesis testing: a data scientist lays out a conjecture about a system in a formal manner. \n",
    "    * Mutually exclusive and contradictory statement about the value of some parameter from a statistical model that characterizes the conjecture.\n",
    "    * A hypothesis is a claim either about the value of a single population characteristic or about the values of several population characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,1000)\n",
    "pdf = np.exp(-0.5*x**2)/np.sqrt(2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "plt.plot(x, pdf, 'b')\n",
    "plt.xlabel('Support', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "plt.ylim((0,.5))\n",
    "plt.title(r'Rejection Regions for $\\alpha=0.05$', fontsize = 20)\n",
    "plt.axvline(-1.96, color='r', ls='--', lw=2.0)\n",
    "plt.axvline(1.96, color='r', ls='--', lw=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
